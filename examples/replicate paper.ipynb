{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce50b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import WildlifeDataset\n",
    "from torchvision import transforms as T\n",
    "import pandas as pd\n",
    "\n",
    "root = '/home/cermavo3/projects/wildlife-experiments/data_256/StripeSpotter'\n",
    "#root = '/home/cermavo3/projects/wildlife-experiments/data_256/Giraffes'\n",
    "metadata = pd.read_csv(f'{root}/annotations.csv', index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def create_distance_matrix(sim, idx_train, idx_test):\n",
    "    '''\n",
    "    Create distance matrix for each of the train / test pair, given input matrix.\n",
    "    Input matrix is product of matching algorithm\n",
    "\n",
    "    Input: upper triangular matrix with shape (n_total, n_total) with zeros on diagonal.\n",
    "    Output: distance matrix of shape (n_test, n_train)     \n",
    "\n",
    "    `'''\n",
    "    sim = sim.copy().astype(np.float32)\n",
    "\n",
    "    if not np.allclose(sim, np.triu(sim)):\n",
    "        raise ValueError('Input matrix needs to be upper triangular.')\n",
    "\n",
    "    if not np.all(np.diag(sim) == 0):\n",
    "        raise ValueError('Input matrix needs to have zeros in diagonal.')\n",
    "\n",
    "\n",
    "    np.fill_diagonal(sim, np.inf)\n",
    "    sim_symetric = np.sum([sim, sim.T], axis=0)\n",
    "    sim_subset = sim_symetric[:, idx_train][idx_test, :]\n",
    "    return sim_subset\n",
    "\n",
    "\n",
    "\n",
    "from features.sift import SIFTFeatures\n",
    "from similarity.descriptors import MatchDescriptors\n",
    "from data.split import SplitWildlife\n",
    "from wildlife_datasets import splits\n",
    "\n",
    "# Grayscale PIL images\n",
    "transform = T.Compose([\n",
    "    T.Resize(size=256),\n",
    "    T.Grayscale(),\n",
    "    T.CenterCrop(size=(224, 224)),\n",
    "])\n",
    "\n",
    "dataset_database = WildlifeDataset(\n",
    "    metadata=metadata,\n",
    "    root=root,\n",
    "    transform=transform,\n",
    "    split=SplitWildlife(splits.ClosedSetSplit(0.8, identity_skip='unknown', seed=666), split='train'),\n",
    ")\n",
    "\n",
    "dataset_query = WildlifeDataset(\n",
    "    metadata=metadata,\n",
    "    root=root,\n",
    "    transform=transform,\n",
    "    split=SplitWildlife(splits.ClosedSetSplit(0.8, identity_skip='unknown', seed=666), split='test')\n",
    ")\n",
    "\n",
    "dataset_all = WildlifeDataset(\n",
    "    metadata=metadata,\n",
    "    root=root,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bf40e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77991814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a0d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044700d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c92c2137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: Matching all pairs in query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [00:07<00:00, 105.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs     : 339076\n",
      "Pairs in chunk  : 339076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 339076/339076 [00:49<00:00, 6912.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from wildlife_datasets import splits\n",
    "\n",
    "\n",
    "root2 = '/home/cermavo3/projects/wildlife-experiments/experiments/matchers_descriptor/runs'\n",
    "path2 = f\"{root2}/StripeSpotter_sift_1-1_Jul19-09-41-55-1540/split-0/similarity.pickle\"\n",
    "\n",
    "\n",
    "splitter = splits.ClosedSetSplit(0.8, identity_skip='unknown', seed=666)\n",
    "idx_train, idx_test = splitter.split(metadata)[0]\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join(path2), 'rb') as handle:\n",
    "    similarity_old = pickle.load(handle)\n",
    "sim_old = similarity_old[0.6]\n",
    "sim2 = create_distance_matrix(sim_old, idx_train, idx_test)\n",
    "\n",
    "\n",
    "matcher2 = SIFTMatcher(device='cuda', thresholds=[0.2, 0.4, 0.6])\n",
    "matcher2.train(dataset_all) # GOOD: This matches old stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b18c00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 7., 2., ..., 6., 4., 1.],\n",
       "       [8., 7., 2., ..., 3., 3., 3.],\n",
       "       [1., 4., 0., ..., 2., 4., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 3., 1., 3.],\n",
       "       [2., 2., 0., ..., 1., 2., 1.],\n",
       "       [1., 1., 3., ..., 1., 5., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8dea294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: Matching query with database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:01<00:00, 105.53it/s]\n",
      "100%|██████████| 656/656 [00:06<00:00, 106.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs     : 107584\n",
      "Pairs in chunk  : 107584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107584/107584 [00:15<00:00, 7007.40it/s]\n"
     ]
    }
   ],
   "source": [
    "matcher = SIFTMatcher(device='cuda', thresholds=[0.2, 0.4, 0.6])\n",
    "matcher.train(dataset_query=dataset_query, dataset_database=dataset_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8b6b7f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 0., ..., 0., 2., 0.],\n",
       "       [8., 7., 2., ..., 3., 0., 3.],\n",
       "       [1., 4., 0., ..., 2., 4., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 2., 3., 3.],\n",
       "       [2., 2., 0., ..., 1., 2., 1.],\n",
       "       [1., 1., 1., ..., 1., 5., 5.]], dtype=float16)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2ab40e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(matcher.similarity[0.6] == similarity[0.6]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f8344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a3b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ac2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import faiss\n",
    "from similarity.base import Similarity\n",
    "\n",
    "\n",
    "def get_faiss_index(d, device='cpu'):\n",
    "    if device == 'cuda':\n",
    "        resource = faiss.StandardGpuResources()\n",
    "        config = faiss.GpuIndexFlatConfig()\n",
    "        config.device = 0\n",
    "        return faiss.GpuIndexFlatL2(resource, d, config)\n",
    "    elif device == 'cpu':\n",
    "        return faiss.IndexFlatL2(d)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid device: {device}')\n",
    "\n",
    "\n",
    "class MatchDescriptors(Similarity):\n",
    "    def __init__(\n",
    "        self,\n",
    "        descriptor_dim: int = 128,\n",
    "        thresholds: tuple[float] = (0.5, ),\n",
    "        device: str = 'cpu',\n",
    "    ):\n",
    "\n",
    "        self.descriptor_dim = descriptor_dim\n",
    "        self.thresholds = thresholds\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def calculate(self, query, database):\n",
    "        iterator = itertools.product(enumerate(query), enumerate(database))\n",
    "        iterator_size = len(query)*len(database)\n",
    "        similarities = {t: np.full((len(query), len(database)), np.nan, dtype=np.float16) for t in self.thresholds}\n",
    "\n",
    "        index = get_faiss_index(d=self.descriptor_dim, device=self.device)\n",
    "        for pair in tqdm(iterator, total=iterator_size, mininterval=1, ncols=100):\n",
    "            (q_idx, q_data), (d_idx, d_data) = pair\n",
    "\n",
    "            if (q_data is None) or (d_data is None):\n",
    "                for t in self.thresholds:\n",
    "                    similarities[t][q_idx, d_idx] = 0\n",
    "\n",
    "            else:\n",
    "                index.reset()\n",
    "                index.add(q_data)\n",
    "                score, idx = index.search(d_data, k=2)\n",
    "                with np.errstate(divide='ignore'):\n",
    "                    ratio = score[:, 0] / score[:, 1]\n",
    "                for t in self.thresholds:\n",
    "                    similarities[t][q_idx, d_idx] = np.sum(ratio < t)\n",
    "\n",
    "        return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1ca29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b307f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6192b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b44b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d093f8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 164/164 [00:01<00:00, 104.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 656/656 [00:06<00:00, 105.32it/s]\n",
      "100%|█████████████████████████████████████████████████████| 107584/107584 [00:16<00:00, 6536.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract set of SIFT local descriptor for each image.\n",
    "extractor = SIFTFeatures()\n",
    "\n",
    "features_query = extractor(dataset_query)\n",
    "features_database = extractor(dataset_database)\n",
    "\n",
    "\n",
    "similarity_func = MatchDescriptors(descriptor_dim=128, thresholds=[0.2, 0.4, 0.6, 0.8], device='cuda')\n",
    "similarity = similarity_func.calculate(\n",
    "    query=features_query,\n",
    "    database=features_database,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38a7e530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 164/164 [00:01<00:00, 104.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████| 656/656 [00:06<00:00, 106.00it/s]\n",
      "100%|█████████████████████████████████████████████████████| 107584/107584 [00:16<00:00, 6556.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract set of SIFT local descriptor for each image.\n",
    "extractor = SIFTFeatures()\n",
    "\n",
    "features_query = extractor(dataset_query)\n",
    "features_database = extractor(dataset_database)\n",
    "\n",
    "\n",
    "similarity_func = MatchDescriptors(descriptor_dim=128, thresholds=[0.2, 0.4, 0.6, 0.8], device='cuda')\n",
    "similarity2 = similarity_func.calculate(\n",
    "    query=features_database,\n",
    "    database=features_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da949675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_query = dataset_query.metadata['identity'].values.astype(str)\n",
    "labels_database = dataset_database.metadata['identity'].values.astype(str)\n",
    "\n",
    "import torch\n",
    "scores, idx = torch.tensor(similarity[0.2], dtype=float).topk(k=1, dim=1, largest=True)\n",
    "pred = labels_database[idx].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1590ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_query = dataset_query.metadata['identity'].values.astype(str)\n",
    "labels_database = dataset_database.metadata['identity'].values.astype(str)\n",
    "\n",
    "import torch\n",
    "scores, idx = torch.tensor(similarity2[0.2].T, dtype=float).topk(k=1, dim=1, largest=True)\n",
    "pred = labels_database[idx].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9905ef59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits = (pred == labels_query)\n",
    "sum(hits) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "266ba67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8963414634146342"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits = (pred == labels_query)\n",
    "sum(hits) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ee4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6ae1446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7195121951219512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits = (pred == labels_query)\n",
    "sum(hits) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f4817eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7439024390243902"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits = (pred == labels_query)\n",
    "sum(hits) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789e034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0f473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b97fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5e9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3e43f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f15e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd774222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_classifier(distance, train_labels):\n",
    "    scores, idx = torch.tensor(distance).topk(k=1, dim=1, largest=False)\n",
    "    return train_labels[idx].flatten(), scores.flatten()\n",
    "\n",
    "\n",
    "def evaluate_closed(distance, train_labels, test_labels):\n",
    "    prediction, score = nn_classifier(distance, train_labels)\n",
    "    return {\n",
    "        'acc': metrics.accuracy(test_labels, prediction),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba847fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89902c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_query = WildlifeDataset(\n",
    "    metadata=metadata,\n",
    "    root=root,\n",
    "    transform=transform,\n",
    "    split=SplitWildlife(splitter, split='test')\n",
    ")\n",
    "\n",
    "\n",
    "# Cosine similarity between deep features\n",
    "extractor = DeepFeatures(device='cuda', model=timm.create_model('swin_tiny_patch4_window7_224', num_classes=0, pretrained=True))\n",
    "\n",
    "similarity_func = CosineSimilarity()\n",
    "\n",
    "features_query = extractor(dataset_query)\n",
    "features_database = extractor(dataset_database)\n",
    "\n",
    "similarity = similarity_func.calculate(\n",
    "    query=features_query,\n",
    "    database=features_database,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f69d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "255add4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "import kornia.feature as KF\n",
    "from data.dataset import WildlifeDataset\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import fcntl\n",
    "import time\n",
    "import errno\n",
    "\n",
    "\n",
    "def create_distance_matrix(sim, idx_train, idx_test):\n",
    "    '''\n",
    "    Create distance matrix for each of the train / test pair, given input matrix.\n",
    "    Input matrix is product of matching algorithm\n",
    "\n",
    "    Input: upper triangular matrix with shape (n_total, n_total) with zeros on diagonal.\n",
    "    Output: distance matrix of shape (n_test, n_train)     \n",
    "\n",
    "    `'''\n",
    "    sim = sim.copy().astype(np.float32)\n",
    "\n",
    "    if not np.allclose(sim, np.triu(sim)):\n",
    "        raise ValueError('Input matrix needs to be upper triangular.')\n",
    "\n",
    "    if not np.all(np.diag(sim) == 0):\n",
    "        raise ValueError('Input matrix needs to have zeros in diagonal.')\n",
    "\n",
    "\n",
    "    np.fill_diagonal(sim, np.inf)\n",
    "    sim_symetric = np.sum([sim, sim.T], axis=0)\n",
    "    sim_subset = sim_symetric[:, idx_train][idx_test, :]\n",
    "    return -sim_subset\n",
    "\n",
    "\n",
    "import pickle\n",
    "def single_file_save(path, similarity_new):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, 'w') as f: # Create empty file to lock on first write.\n",
    "            pass\n",
    "\n",
    "    for i in range(60): # Try for one hour\n",
    "        try:\n",
    "            with open(path, 'rb+') as file:\n",
    "                fcntl.flock(file, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "                if os.path.getsize(path) == 0:\n",
    "                    similarity = similarity_new\n",
    "                else:\n",
    "                    #similarity = np.load(file, allow_pickle='TRUE').item()\n",
    "                    similarity = pickle.load(file)\n",
    "                    for key in similarity_new.keys():\n",
    "                        similarity[key] = np.sum([similarity[key], similarity_new[key]], axis=0)\n",
    "                    file.seek(0)\n",
    "\n",
    "                #np.save(file, similarity)\n",
    "                pickle.dump(similarity, file, pickle.HIGHEST_PROTOCOL)\n",
    "                fcntl.flock(file, fcntl.LOCK_UN)\n",
    "                break\n",
    "        except (OSError, IOError) as e:\n",
    "            if e.errno != errno.EAGAIN:\n",
    "                raise\n",
    "            print('File locked - waiting...')\n",
    "            time.sleep(60)\n",
    "\n",
    "\n",
    "def get_faiss_index(d, device='cpu'):\n",
    "    if device == 'cuda':\n",
    "        resource = faiss.StandardGpuResources()\n",
    "        config = faiss.GpuIndexFlatConfig()\n",
    "        config.device = 0\n",
    "        return faiss.GpuIndexFlatL2(resource, d, config)\n",
    "    elif device == 'cpu':\n",
    "        return faiss.IndexFlatL2(d)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid device: {device}')\n",
    "\n",
    "\n",
    "def batched(iterable, n):\n",
    "    '''\n",
    "    Batch data into tuples of length n. The last batch may be shorter.\n",
    "    Example: batched('ABCDEFG', 3) --> ABC DEF G\n",
    "    '''\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(itertools.islice(it, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def prepare_pair_batches(query, database=None, batch_size=128, chunk=1, chunk_total=1):\n",
    "    '''\n",
    "    Prepares batches of data pairs given query and optionally the database. \n",
    "    \n",
    "    Returns an iterator that iterates through the batches. Optionally, it can\n",
    "    be splited to chunks for better memory optimization and parallelization.\n",
    "    '''\n",
    "    if database:\n",
    "        pair_total = len(query)*len(database)\n",
    "        pair_iterator = itertools.product(enumerate(query), enumerate(database))\n",
    "    else:\n",
    "        pair_total = math.comb(len(query), 2)\n",
    "        pair_iterator = itertools.combinations(enumerate(query), 2)\n",
    "\n",
    "    batch_total = int(np.ceil(pair_total / batch_size))\n",
    "    batch_iterator = batched(pair_iterator, batch_size)\n",
    "\n",
    "    batches = np.array_split(np.arange(batch_total), chunk_total)[chunk-1]\n",
    "    batch_min, batch_max = batches[0], batches[-1] + 1\n",
    "    iterator = itertools.islice(batch_iterator, batch_min, batch_max)\n",
    "\n",
    "    print(f'Total pairs     : {pair_total}')\n",
    "    print(f'Total batches   : {batch_total}')\n",
    "    print(f'Batches in chunk: {len(batches)}')\n",
    "    return iterator, len(batches)\n",
    "\n",
    "\n",
    "def prepare_pairs(query, database=None, batch_size=128, chunk=1, chunk_total=1):\n",
    "    '''\n",
    "    Prepares data pairs given query and optionally the database. \n",
    "    \n",
    "    Returns an iterator that iterates through the batches. Optionally, it can\n",
    "    be splited to chunks for better memory optimization and parallelization.\n",
    "    '''\n",
    "    if database:\n",
    "        pair_total = len(query)*len(database)\n",
    "        pair_iterator = itertools.product(enumerate(query), enumerate(database))\n",
    "    else:\n",
    "        pair_total = math.comb(len(query), 2)\n",
    "        pair_iterator = itertools.combinations(enumerate(query), 2)\n",
    "\n",
    "    pairs = np.array_split(np.arange(pair_total), chunk_total)[chunk-1]\n",
    "    pair_min, pair_max = pairs[0], pairs[-1] + 1\n",
    "    iterator = itertools.islice(pair_iterator, pair_min, pair_max)\n",
    "\n",
    "    print(f'Total pairs     : {pair_total}')\n",
    "    print(f'Pairs in chunk  : {len(pairs)}')\n",
    "    return iterator, len(pairs)\n",
    "\n",
    "\n",
    "def compose_similarity(folders):\n",
    "    '''\n",
    "    Create similarity matrix given folders with similarity matrix chunks.\n",
    "    '''\n",
    "    data = []\n",
    "    for folder in folders:\n",
    "        path = os.path.join(folder, 'similarity.pickle')\n",
    "        data_chunk = np.load(path, allow_pickle='TRUE').item()\n",
    "        data.append(data_chunk)\n",
    "\n",
    "    similarity = {}\n",
    "    for key in data_chunk.keys():\n",
    "        similarity[key] = np.sum([d[key] for d in data], axis=0)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "class LOFTRMatcher():\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: str ='cuda',\n",
    "        pretrained: str ='outdoor',\n",
    "        thresholds: tuple[float] = (0.99, ),\n",
    "        batch_size: int = 128,\n",
    "        chunk: int = 1,\n",
    "        chunk_total: int = 1,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.matcher = KF.LoFTR(pretrained=pretrained).to(device)\n",
    "        self.thresholds = thresholds\n",
    "        self.batch_size = batch_size\n",
    "        if chunk > chunk_total:\n",
    "            raise ValueError('Current chunk is larger that chunk total.')\n",
    "        self.chunk = chunk\n",
    "        self.chunk_total = chunk_total\n",
    "        self.similarity = None\n",
    "\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        dataset_query: WildlifeDataset,\n",
    "        dataset_database: WildlifeDataset | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if dataset_database:\n",
    "            print('Matching query with database')\n",
    "            query = [i[0] for i in dataset_query]\n",
    "            database = [i[0] for i in dataset_database]\n",
    "            self.similarity = {t: np.zeros((len(query), len(database)), dtype=np.float16) for t in self.thresholds}\n",
    "        else:\n",
    "            print('Matching all pairs in query')\n",
    "            query = [i[0] for i in dataset_query]\n",
    "            database = None\n",
    "            self.similarity = {t: np.zeros((len(query), len(query)), dtype=np.float16) for t in self.thresholds}\n",
    "\n",
    "        iterator, iterator_size = prepare_pair_batches(\n",
    "            query = query,\n",
    "            database = database,\n",
    "            batch_size = self.batch_size,\n",
    "            chunk = self.chunk,\n",
    "            chunk_total = self.chunk_total\n",
    "        )\n",
    "\n",
    "        for pair_batch in tqdm(iterator, total=iterator_size, mininterval=1):\n",
    "            a, b = zip(*pair_batch)\n",
    "            a_idx, a_data = list(zip(*a))\n",
    "            b_idx, b_data = list(zip(*b))\n",
    "            input_dict = {\n",
    "                \"image0\": torch.stack(a_data).to(self.device),\n",
    "                \"image1\": torch.stack(b_data).to(self.device),\n",
    "            }\n",
    "            with torch.inference_mode():\n",
    "                correspondences = self.matcher(input_dict)\n",
    "\n",
    "            batch_idx = correspondences['batch_indexes'].cpu().numpy()\n",
    "            confidence = correspondences['confidence'].cpu().numpy()\n",
    "            for t in self.thresholds:\n",
    "                series = pd.Series(confidence > t)\n",
    "                for j, group in series.groupby(batch_idx):\n",
    "                    self.similarity[t][a_idx[j], b_idx[j]] = group.sum()\n",
    "\n",
    "\n",
    "    def save(self, folder, name='similarity.pickle', **kwargs):\n",
    "        if self.similarity:\n",
    "            with open(os.path.join(folder, name), 'wb') as handle:\n",
    "                pickle.dump(self.similarity, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            #np.save(os.path.join(folder, name), self.similarity)\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path, 'rb') as handle:\n",
    "            self.similarity = pickle.load(handle)\n",
    "        #self.similarity = np.load(path, allow_pickle='TRUE').item()\n",
    "\n",
    "\n",
    "class DescriptorMatcher():\n",
    "    def __init__(\n",
    "        self,\n",
    "        descriptor_function = None,\n",
    "        descriptor_dim: int = 128,\n",
    "        max_keypoints: int | None = None,\n",
    "        thresholds: tuple[float] = (0.5, ),\n",
    "        device: str = 'cpu',\n",
    "        chunk: int = 1,\n",
    "        chunk_total: int = 1,\n",
    "        joint_save: str | None = None,\n",
    "    ):\n",
    "    \n",
    "        self.descriptor_function = descriptor_function\n",
    "        self.descriptor_dim = descriptor_dim\n",
    "        self.max_keypoints = max_keypoints\n",
    "        self.thresholds = thresholds\n",
    "        self.device = device\n",
    "        if chunk > chunk_total:\n",
    "            raise ValueError('Current chunk is larger that chunk total.')\n",
    "        self.chunk = chunk\n",
    "        self.chunk_total = chunk_total\n",
    "        self.joint_save = joint_save\n",
    "        self.similarity = None\n",
    "\n",
    "    def get_descriptors(self, dataset):\n",
    "        if self.descriptor_function:\n",
    "            return self.descriptor_function(dataset)\n",
    "        else:\n",
    "            raise ValueError('No descriptor function provided.')\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        dataset_query: WildlifeDataset,\n",
    "        dataset_database: WildlifeDataset | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if dataset_database:\n",
    "            print('Mode: Matching query with database')\n",
    "            query = self.get_descriptors(dataset_query)\n",
    "            database = self.get_descriptors(dataset_database)\n",
    "            self.similarity = {t: np.zeros((len(query), len(database)), dtype=np.float16) for t in self.thresholds}\n",
    "        else:\n",
    "            print('Mode: Matching all pairs in query')\n",
    "            query = self.get_descriptors(dataset_query)\n",
    "            database = None\n",
    "            self.similarity = {t: np.zeros((len(query), len(query)), dtype=np.float16) for t in self.thresholds}\n",
    "\n",
    "        iterator, iterator_size = prepare_pairs(\n",
    "            query = query,\n",
    "            database = database,\n",
    "            chunk = self.chunk,\n",
    "            chunk_total = self.chunk_total\n",
    "        )\n",
    "\n",
    "        index = get_faiss_index(d=self.descriptor_dim, device=self.device)\n",
    "        for (a_idx, a_data), (b_idx, b_data) in tqdm(iterator, total=iterator_size, mininterval=1):\n",
    "            if (a_data is None) or (b_data is None):\n",
    "                continue\n",
    "            else:\n",
    "                index.reset()\n",
    "                index.add(a_data)\n",
    "                score, idx = index.search(b_data, k=2)\n",
    "                with np.errstate(divide='ignore'):\n",
    "                    ratio = score[:, 0] / score[:, 1]\n",
    "                for t in self.thresholds:\n",
    "                    self.similarity[t][a_idx, b_idx] = np.sum(ratio < t)\n",
    "\n",
    "\n",
    "    def save(self, folder, name='similarity.pickle', **kwargs):\n",
    "        if not self.similarity:\n",
    "            return\n",
    "\n",
    "        if self.joint_save:\n",
    "            single_file_save(self.joint_save, self.similarity)\n",
    "        else:\n",
    "            with open(os.path.join(folder, name), 'wb') as handle:\n",
    "                pickle.dump(self.similarity, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path, 'rb') as handle:\n",
    "            self.similarity = pickle.load(handle)\n",
    "\n",
    "class SIFTMatcher(DescriptorMatcher):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.descriptor_dim = 128\n",
    "\n",
    "    def get_descriptors(self, dataset):\n",
    "        if self.max_keypoints:\n",
    "            sift = cv2.SIFT_create(nfeatures=self.max_keypoints)\n",
    "        else:\n",
    "            sift = cv2.SIFT_create()\n",
    "\n",
    "        descriptors = []\n",
    "        for img, y in tqdm(dataset, mininterval=1):\n",
    "            keypoint, d = sift.detectAndCompute(np.array(img), None)\n",
    "            if len(keypoint) <= 1:\n",
    "                descriptors.append(None)\n",
    "            else:\n",
    "                descriptors.append(d)\n",
    "        return descriptors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
