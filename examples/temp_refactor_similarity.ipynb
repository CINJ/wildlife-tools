{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb56c0b4",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176f94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data.dataset import WildlifeDataset\n",
    "from torchvision import transforms as T\n",
    "from features.utils import save_features, load_features\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "root = '/home/cermavo3/projects/wildlife-experiments/data_256/StripeSpotter'\n",
    "metadata = pd.read_csv(f'{root}/annotations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d217d3f",
   "metadata": {},
   "source": [
    "## Deep features - Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0404e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "from matching.utils import cosine_similarity\n",
    "\n",
    "deep_features = load_features('temp/stripespotter_deep_features.pickle')\n",
    "deep_sim = cosine_similarity(deep_features, deep_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a469ca",
   "metadata": {},
   "source": [
    "## Number of correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de19f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    '''\n",
    "    Batch data into tuples of length n. The last batch may be shorter.\n",
    "    Example: batched('ABCDEFG', 3) --> ABC DEF G\n",
    "    '''\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(itertools.islice(it, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def get_query_iterator(query_idx, query, database):\n",
    "    ''' Generator of query-database pairs for given query data point. '''\n",
    "    for database_idx, database_data in enumerate(database):\n",
    "        yield (query_idx, query[query_idx]), (database_idx, database_data)\n",
    "\n",
    "\n",
    "def create_iterators(query, database, batch_size = None, chunk=1, chunk_total=1):\n",
    "    '''\n",
    "    Given query and database, create list of pair iterators one for each item in query.\n",
    "    If batch size is given, add batch iterator before the pair iterator.\n",
    "    If chunks are given, split query in chunks.\n",
    "    '''\n",
    "    subset = np.array_split(np.arange(len(query)), chunk_total)[chunk-1]\n",
    "\n",
    "    iterators = []\n",
    "    for i in subset:\n",
    "        iterator = get_query_iterator(i, query, database)\n",
    "        if batch_size:\n",
    "            iterator = batched(iterator, batch_size)\n",
    "        iterators.append((i, iterator))\n",
    "    return iterators\n",
    "\n",
    "\n",
    "def get_faiss_index(d, device='cpu'):\n",
    "    if device == 'cuda':\n",
    "        resource = faiss.StandardGpuResources()\n",
    "        config = faiss.GpuIndexFlatConfig()\n",
    "        config.device = 0\n",
    "        return faiss.GpuIndexFlatL2(resource, d, config)\n",
    "    elif device == 'cpu':\n",
    "        return faiss.IndexFlatL2(d)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b067f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6202c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                 | 0/50 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "class DescriptorMatcher():\n",
    "    def __init__(\n",
    "        self,\n",
    "        descriptor_dim: int = 128,\n",
    "        thresholds: tuple[float] = (0.5, ),\n",
    "        device: str = 'cpu',\n",
    "        chunk: int = 1,\n",
    "        chunk_total: int = 1,\n",
    "        joint_save: str | None = None,\n",
    "    ):\n",
    "\n",
    "        self.descriptor_dim = descriptor_dim\n",
    "        self.thresholds = thresholds\n",
    "        self.device = device\n",
    "        if chunk > chunk_total:\n",
    "            raise ValueError('Current chunk is larger that chunk total.')\n",
    "        self.chunk = chunk\n",
    "        self.chunk_total = chunk_total\n",
    "        self.joint_save = joint_save\n",
    "        self.similarity = None\n",
    "\n",
    "self = DescriptorMatcher()\n",
    "features = load_features('temp/stripespotter_sp_features.pickle')\n",
    "database = load_features('temp/stripespotter_sp_features.pickle')[:500]\n",
    "query = load_features('temp/stripespotter_sp_features.pickle')[:50]\n",
    "\n",
    "iterators = create_iterators(\n",
    "    query = query,\n",
    "    database = query,\n",
    "    chunk = self.chunk,\n",
    "    chunk_total = self.chunk_total\n",
    ")\n",
    "\n",
    "idx, iterator = iterators[0]\n",
    "print(idx)\n",
    "\n",
    "\n",
    "#similarity = {t: defaultdict(lambda: np.zeros(len(database), dtype=np.float16)) for t in self.thresholds}\n",
    "similarity = {t: {} for t in self.thresholds}\n",
    "index = get_faiss_index(d=256, device=self.device)\n",
    "for query_idx, iterator in tqdm(iterators):\n",
    "    for t in self.thresholds:\n",
    "        similarity[t][query_idx] = np.zeros(len(database), dtype=np.float16)\n",
    "\n",
    "    for (_, query_data), (database_idx, database_data) in iterator:\n",
    "        \n",
    "        # If no decriptors were found\n",
    "        if (query_data is None) or (database_data is None):\n",
    "            print('xxxz')\n",
    "            continue\n",
    "\n",
    "        index.reset()\n",
    "        index.add(database_data)\n",
    "        score, idx = index.search(query_data, k=2)\n",
    "        with np.errstate(divide='ignore'):\n",
    "            ratio = score[:, 0] / score[:, 1]\n",
    "\n",
    "        for t in self.thresholds:\n",
    "            similarity[t][query_idx][database_idx] = np.sum(ratio < t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8af7e7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: {0: array([42.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.], dtype=float16)}}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8d14f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "index = get_faiss_index(d=256, device=self.device)\n",
    "\n",
    "similarities = {t: defaultdict(lambda: np.zeros(len(database), dtype=np.float16)) for t in self.thresholds}\n",
    "for query_idx, iterator in tqdm(iterators):\n",
    "    matches = np.zeros(len(database), dtype=np.float16)\n",
    "    for query_data, (database_idx, database_data) in iterator:\n",
    "        if (query_data is None) or (database_data is None):\n",
    "            continue\n",
    "\n",
    "        index.reset()\n",
    "        index.add(database_data)\n",
    "        score, idx = index.search(query_data, k=2)\n",
    "        with np.errstate(divide='ignore'):\n",
    "            ratio = score[:, 0] / score[:, 1]\n",
    "        for t in self.thresholds:\n",
    "            similarities[t][query_idx][database_data] = np.sum(ratio < t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb4aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f281868",
   "metadata": {},
   "source": [
    "## V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5c9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2ad4394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = load_features('temp/stripespotter_sp_features.pickle')[:500]\n",
    "query = load_features('temp/stripespotter_sp_features.pickle')[:50]\n",
    "\n",
    "\n",
    "chunk_q_total = 1\n",
    "chunk_q = 1\n",
    "chunk_d_total = 1\n",
    "chunk_d = 1\n",
    "\n",
    "q_subset = np.array_split(np.arange(len(query)), chunk_q_total)[chunk_q-1]\n",
    "d_subset = np.array_split(np.arange(len(database)), chunk_d_total)[chunk_d-1]\n",
    "\n",
    "iterator = itertools.product(enumerate(q_subset), enumerate(d_subset))\n",
    "iterator_size = len(q_subset)*len(d_subset)\n",
    "iterator = add_iterator_data(iterator, query, database)\n",
    "\n",
    "\n",
    "batch_size = None\n",
    "if batch_size:\n",
    "    iterator = batched(iterator, batch_size)\n",
    "\n",
    "\n",
    "similarity = {t: np.zeros((len(query), len(database)), dtype=np.float16) for t in self.thresholds}\n",
    "\n",
    "\n",
    "\n",
    "for (q_idx, q_data), (d_idx, d_data) in tqdm(iterator, total=iterator_size, mininterval=1):\n",
    "    if (d_data is None) or (q_data is None):\n",
    "        for t in self.thresholds:\n",
    "            similarity[t][q_idx][d_idx] = 0\n",
    "    else:\n",
    "        index.reset()\n",
    "        index.add(d_data)\n",
    "        score, idx = index.search(q_data, k=2)\n",
    "        with np.errstate(divide='ignore'):\n",
    "            ratio = score[:, 0] / score[:, 1]\n",
    "\n",
    "        for t in self.thresholds:\n",
    "            similarity[t][q_idx][d_idx] = np.sum(ratio < t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210fc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a6fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c31094db",
   "metadata": {},
   "source": [
    "### Number of LOFT correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4bae947",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import sys\n",
    "sys.path.append('/home/cermavo3/projects/wildlife-tools/models')\n",
    "sys.path.append('/home/cermavo3/projects/wildlife-tools')\n",
    "from matchers import LOFTRMatcher\n",
    "matcher = LOFTRMatcher(thresholds=[0.73])\n",
    "dataset = WildlifeDataset(metadata.iloc[:50], root=root, transform=transform)\n",
    "sim_v1 = matcher.train(dataset_query=dataset, dataset_database=dataset)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ef6a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from data.dataset import WildlifeDataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "root = '/home/cermavo3/projects/wildlife-experiments/data_256/StripeSpotter'\n",
    "metadata = pd.read_csv(f'{root}/annotations.csv')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(size=256),\n",
    "    T.CenterCrop(size=(224, 224)),\n",
    "    T.Grayscale(),\n",
    "    T.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21d6b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee357079",
   "metadata": {},
   "outputs": [],
   "source": [
    "'matcher': {\n",
    "    'method': 'descriptor',\n",
    "    'device': 'cuda',\n",
    "    'num_dim': 256,\n",
    "}\n",
    "\n",
    "    \n",
    "pipeline: [\n",
    "    'load_data',\n",
    "    'matcher',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'matcher': {\n",
    "    'method': 'descriptor',\n",
    "    'device': 'cuda',\n",
    "    'num_dim': 256,\n",
    "}\n",
    "\n",
    "'query_dataset': {\n",
    "    'method': 'folder',\n",
    "    'root': '/home/cermavo3/projects/wildlife-experiments/data_256/StripeSpotter',\n",
    "    'transform': {\n",
    "    }\n",
    "}\n",
    "\n",
    "pipeline: [\n",
    "    'load_data',\n",
    "    'matcher' {\n",
    "        'query' : {\n",
    "            'query_dataset'\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1947913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match -> similarity\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d7040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93afe3e1",
   "metadata": {},
   "source": [
    "## V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d86f7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_iterator_data(iterator, data_a, data_b):\n",
    "    ''' Convert data-index to data-point given pair iterator and source data. '''\n",
    "    for (a_idx, a), (b_idx, b) in iterator:\n",
    "        yield (a_idx, data_a[a]), (b_idx, data_b[b])\n",
    "        \n",
    "\n",
    "def get_indexes(array, chunk, chunk_total):\n",
    "    return np.array_split(np.arange(len(features)), chunk_total)[chunk-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b22165",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = LOFTRMatcher(device='cuda', thresholds=[0.73])\n",
    "\n",
    "\n",
    "dataset = WildlifeDataset(metadata.iloc[:50], root=root, transform=transform)\n",
    "\n",
    "# Inputs\n",
    "\n",
    "d_chunk = 1\n",
    "d_chunk_total = 1\n",
    "d_subset = np.array_split(np.arange(len(database)), chunk_d_total)[chunk_d-1]\n",
    "database = [i[0] for i in dataset]\n",
    "\n",
    "query = [i[0] for i in dataset]\n",
    "q_chunk = 1\n",
    "q_chunk_total = 1\n",
    "q_subset = np.array_split(np.arange(len(query)), chunk_q_total)[chunk_q-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7026e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadedDataset():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1ab3da3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:21<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import kornia.feature as KF\n",
    "class LOFTRMatcher():\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: str ='cuda',\n",
    "        pretrained: str ='outdoor',\n",
    "        thresholds: tuple[float] = (0.99, ),\n",
    "        batch_size: int = 128,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.matcher = KF.LoFTR(pretrained=pretrained).to(device)\n",
    "        self.thresholds = thresholds\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "    def match(self, query, database):\n",
    "        iterator = batched(itertools.product(enumerate(query), enumerate(database)), self.batch_size)\n",
    "        iterator_size = int(np.ceil(len(query)*len(database) / self.batch_size))\n",
    "        similarity = {t: np.full((len(query), len(database)), np.nan, dtype=np.float16) for t in self.thresholds}\n",
    "\n",
    "        for pair_batch in tqdm(iterator, total=iterator_size, mininterval=1):\n",
    "            q, d = zip(*pair_batch)\n",
    "            q_idx, q_data = list(zip(*q))\n",
    "            d_idx, d_data = list(zip(*d))\n",
    "            input_dict = {\n",
    "                \"image0\": torch.stack(a_data).to(self.device),\n",
    "                \"image1\": torch.stack(b_data).to(self.device),\n",
    "            }\n",
    "            with torch.inference_mode():\n",
    "                correspondences = self.matcher(input_dict)\n",
    "\n",
    "            batch_idx = correspondences['batch_indexes'].cpu().numpy()\n",
    "            confidence = correspondences['confidence'].cpu().numpy()\n",
    "            for t in self.thresholds:\n",
    "                series = pd.Series(confidence > t)\n",
    "                for j, group in series.groupby(batch_idx):\n",
    "                    similarity[t][q_idx[j], d_idx[j]] = group.sum()\n",
    "        return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea18da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "13d8bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (q_store_idx, q_idx), (d_store_idx, d_idx) in iterator:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e9bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ed5dff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, d = zip(*pair_batch)\n",
    "q_idx, q_data = list(zip(*[(i, query[j]) for i, j in q]))\n",
    "b_idx, database_data = list(zip(*[(i, database[j]) for i, j in d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e6876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6784df92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(matcher.similarity[0.73] == similarity[0.73]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803b021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d57586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d7191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafca18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query\n",
    "load_descriptors()\n",
    "Chunks\n",
    "\n",
    "database\n",
    "#load_descriptor()\n",
    "Chunks\n",
    "\n",
    "DescriptorMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8d494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3df68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
